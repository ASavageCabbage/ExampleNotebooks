{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy Neural Net (Multilayer Perceptron with Autograd Backpropogation)\n",
    "\n",
    "Goal: Train a multilayer perceptron to recognize numbers from images using the MNIST database.\n",
    "\n",
    "Made with help from:\n",
    "- https://nextjournal.com/gkoehler/pytorch-mnist\n",
    "- https://github.com/WatChMaL/ExampleNotebooks/blob/master/MNIST%20MLP.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the MNIST database\n",
    "\n",
    "Initializing a trainLoader and testLoader for training and evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "# Set parameters for dataset to fetch\n",
    "trainBatchSize = 64\n",
    "testBatchSize = 1000\n",
    "\n",
    "# Fetch datasets (28x28 pixels)\n",
    "trainLoader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('/files/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor()])),\n",
    "  batch_size=trainBatchSize, shuffle=True)\n",
    "\n",
    "testLoader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('/files/', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor()])),\n",
    "  batch_size=testBatchSize, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot some of the test data for fun:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "examples = enumerate(testLoader)\n",
    "idx, (exampleData, exampleTargets) = next(examples)\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(exampleData[i][0], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Ground Truth: {}\".format(exampleTargets[i]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Implementation\n",
    "\n",
    "Time to implement the neural net. Somewhat arbitrarily, I decided to use an MLP with 2 hidden layers of 500, then 100 neurons, with an input layer of 28x28 = 784 neurons and an output layer of 10 neurons corresponding to the digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    # Constructor initializes two hidden layers with neuron counts specified by the user\n",
    "    def __init__(self, L1, L2):\n",
    "        super(MLP, self).__init__()\n",
    "        # input -(W1)-> L1 -(W2)-> L2 -(W3)-> output\n",
    "        self._classifier = nn.Sequential(\n",
    "            nn.Linear(28*28, L1), nn.ReLU(),\n",
    "            nn.Linear(L1, L2), nn.ReLU(),\n",
    "            nn.Linear(L2, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Flatten 2d input into 1d array:\n",
    "        x_flat = x.view(-1, np.prod(x.size()[1:]))\n",
    "        # Run through classifier\n",
    "        return self._classifier(x_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define a BLOB class to organize training objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty class definition\n",
    "class BLOB:\n",
    "    pass\n",
    "# Dynamic attribute allocation\n",
    "blob = BLOB()\n",
    "blob.net = MLP(256, 128) # HERE SHE IS\n",
    "blob.errFn = nn.CrossEntropyLoss() # Error-defining function is softmax\n",
    "blob.optim = torch.optim.Adam(blob.net.parameters()) # Adam optimizer algorithm\n",
    "blob.iter = 0 # training iteration number\n",
    "blob.data = None # data for training/analysis\n",
    "blob.expect = None # correct values\n",
    "\n",
    "# Forward evolution function\n",
    "# REQUIRES: blob arg must have all attributes set\n",
    "def forward(blob, train=True):\n",
    "    with torch.set_grad_enabled(train):\n",
    "        # Get prediction from MLP\n",
    "        pred = blob.net(blob.data)\n",
    "        # Training stuff\n",
    "        loss = -1\n",
    "        if blob.expect is not None:\n",
    "            # blob.expect.requires_grad(False) # Something odd is happening on this line, commenting it out seems to work\n",
    "            loss = blob.errFn(pred, blob.expect)\n",
    "        # Another dynamically assigned attribute\n",
    "        blob.loss = loss\n",
    "        \n",
    "        pred = torch.argmax(pred, dim=1)\n",
    "        correctTensor = (pred == blob.expect) # Tensor of booleans corresponding to correct predictions\n",
    "        acc = correctTensor.sum().item() / float(pred.nelement()) # Accuracy of MLP\n",
    "        \n",
    "        return {'prediction' : pred.detach().numpy(),\n",
    "                'loss'       : loss.detach().item(),\n",
    "                'accuracy'   : acc}\n",
    "    \n",
    "# Backprop initialization\n",
    "# REQUIRES: blob arg must first have been run through forward\n",
    "def backward(blob):\n",
    "    blob.optim.zero_grad()\n",
    "    blob.loss.backward()\n",
    "    blob.optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Now for the training loop. This first part is just to figure out what the heck trainLoader actually contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:\n",
      "0\n",
      "data:\n",
      "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([9, 6, 3, 9, 9, 9, 6, 0, 4, 1, 2, 7, 6, 4, 9, 4, 8, 2, 7, 5, 6, 4, 6, 8,\n",
      "        8, 7, 3, 8, 1, 2, 6, 2, 9, 4, 4, 1, 3, 3, 1, 9, 8, 1, 3, 8, 5, 4, 3, 4,\n",
      "        9, 7, 5, 2, 0, 2, 2, 1, 4, 7, 0, 8, 4, 2, 0, 5])]\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(trainLoader):\n",
    "    if (i > 0):\n",
    "        break\n",
    "    print(\"i:\")\n",
    "    print(i)\n",
    "    print(\"data:\")\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now for the real training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 \t| Loss: 2.3064396381378174 \t| Accuracy: 0.09375\n",
      "Iteration: 10 \t| Loss: 1.9630744457244873 \t| Accuracy: 0.515625\n",
      "Iteration: 20 \t| Loss: 1.278629183769226 \t| Accuracy: 0.671875\n",
      "Iteration: 30 \t| Loss: 0.7999889850616455 \t| Accuracy: 0.765625\n",
      "Iteration: 40 \t| Loss: 0.8534802794456482 \t| Accuracy: 0.734375\n",
      "Iteration: 50 \t| Loss: 0.4231760501861572 \t| Accuracy: 0.890625\n",
      "Iteration: 60 \t| Loss: 0.4366120994091034 \t| Accuracy: 0.859375\n",
      "Iteration: 70 \t| Loss: 0.42479944229125977 \t| Accuracy: 0.890625\n",
      "Iteration: 80 \t| Loss: 0.4140319228172302 \t| Accuracy: 0.84375\n",
      "Iteration: 90 \t| Loss: 0.3230949938297272 \t| Accuracy: 0.921875\n",
      "Iteration: 100 \t| Loss: 0.2714061439037323 \t| Accuracy: 0.953125\n",
      "Iteration: 110 \t| Loss: 0.41483134031295776 \t| Accuracy: 0.859375\n",
      "Iteration: 120 \t| Loss: 0.45617344975471497 \t| Accuracy: 0.890625\n",
      "Iteration: 130 \t| Loss: 0.5029744505882263 \t| Accuracy: 0.890625\n",
      "Iteration: 140 \t| Loss: 0.5219317674636841 \t| Accuracy: 0.90625\n",
      "Iteration: 150 \t| Loss: 0.18307410180568695 \t| Accuracy: 0.953125\n",
      "Iteration: 160 \t| Loss: 0.2832739055156708 \t| Accuracy: 0.9375\n",
      "Iteration: 170 \t| Loss: 0.293120414018631 \t| Accuracy: 0.890625\n",
      "Iteration: 180 \t| Loss: 0.24138785898685455 \t| Accuracy: 0.90625\n",
      "Iteration: 190 \t| Loss: 0.19987213611602783 \t| Accuracy: 0.96875\n",
      "Iteration: 200 \t| Loss: 0.2509611248970032 \t| Accuracy: 0.90625\n",
      "Iteration: 210 \t| Loss: 0.2872120141983032 \t| Accuracy: 0.9375\n",
      "Iteration: 220 \t| Loss: 0.27381181716918945 \t| Accuracy: 0.921875\n",
      "Iteration: 230 \t| Loss: 0.1861172616481781 \t| Accuracy: 0.953125\n",
      "Iteration: 240 \t| Loss: 0.31294241547584534 \t| Accuracy: 0.921875\n",
      "Iteration: 250 \t| Loss: 0.20470848679542542 \t| Accuracy: 0.953125\n",
      "Iteration: 260 \t| Loss: 0.40607887506484985 \t| Accuracy: 0.921875\n",
      "Iteration: 270 \t| Loss: 0.14701859652996063 \t| Accuracy: 0.953125\n",
      "Iteration: 280 \t| Loss: 0.4455714225769043 \t| Accuracy: 0.890625\n",
      "Iteration: 290 \t| Loss: 0.43589967489242554 \t| Accuracy: 0.875\n",
      "Iteration: 300 \t| Loss: 0.22183629870414734 \t| Accuracy: 0.9375\n",
      "Iteration: 310 \t| Loss: 0.1484198272228241 \t| Accuracy: 0.953125\n",
      "Iteration: 320 \t| Loss: 0.2983500361442566 \t| Accuracy: 0.953125\n",
      "Iteration: 330 \t| Loss: 0.1832415610551834 \t| Accuracy: 0.9375\n",
      "Iteration: 340 \t| Loss: 0.1437387466430664 \t| Accuracy: 0.984375\n",
      "Iteration: 350 \t| Loss: 0.3947833776473999 \t| Accuracy: 0.875\n",
      "Iteration: 360 \t| Loss: 0.3866809010505676 \t| Accuracy: 0.828125\n",
      "Iteration: 370 \t| Loss: 0.5952752828598022 \t| Accuracy: 0.84375\n",
      "Iteration: 380 \t| Loss: 0.1449039727449417 \t| Accuracy: 0.953125\n",
      "Iteration: 390 \t| Loss: 0.24101941287517548 \t| Accuracy: 0.953125\n",
      "Iteration: 400 \t| Loss: 0.2496783286333084 \t| Accuracy: 0.96875\n",
      "Iteration: 410 \t| Loss: 0.16483330726623535 \t| Accuracy: 0.984375\n",
      "Iteration: 420 \t| Loss: 0.07089630514383316 \t| Accuracy: 1.0\n",
      "Iteration: 430 \t| Loss: 0.11366242915391922 \t| Accuracy: 0.96875\n",
      "Iteration: 440 \t| Loss: 0.20245227217674255 \t| Accuracy: 0.9375\n",
      "Iteration: 450 \t| Loss: 0.16407448053359985 \t| Accuracy: 0.96875\n",
      "Iteration: 460 \t| Loss: 0.16884608566761017 \t| Accuracy: 0.953125\n",
      "Iteration: 470 \t| Loss: 0.14876261353492737 \t| Accuracy: 0.96875\n",
      "Iteration: 480 \t| Loss: 0.088211789727211 \t| Accuracy: 0.96875\n",
      "Iteration: 490 \t| Loss: 0.07279844582080841 \t| Accuracy: 0.96875\n"
     ]
    }
   ],
   "source": [
    "# Set MLP to training mode\n",
    "blob.net.train()\n",
    "# Set number of training iterations\n",
    "maxIter = 500\n",
    "# Set reporting interval\n",
    "repIter = 10\n",
    "# Training loop\n",
    "for i, data in enumerate(trainLoader):\n",
    "    blob.iter = i\n",
    "    # data consists of a list of two tensors [img data, values]\n",
    "    # unpack data into appropriate attributes in blob\n",
    "    blob.data, blob.expect = data\n",
    "    # test MLP\n",
    "    res = forward(blob)\n",
    "    # Report if current iteration is a multiple of the reporting interval\n",
    "    if (blob.iter % repIter) == 0:\n",
    "        print('Iteration:', blob.iter,'\\t| Loss:', res['loss'],'\\t| Accuracy:',res['accuracy'])\n",
    "    if (blob.iter + 1 >= maxIter):\n",
    "        break\n",
    "    # Backprop\n",
    "    backward(blob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Now to test the MLP on data it hasn't seen before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\taccuracy mean 0.9481609808102346 std 0.028033261387632912\n",
      "TEST\taccuracy mean 0.9452999999999999 std 0.005060632371551982\n"
     ]
    }
   ],
   "source": [
    "def evaluate(blob, loader):\n",
    "    # Set MLP to evalutation mode\n",
    "    blob.net.eval()\n",
    "    # Initialize result containers\n",
    "    accuracy, expected, prediction = [], [], []\n",
    "    for i, data in enumerate(loader):\n",
    "        blob.data, blob.expect = data\n",
    "        res = forward(blob)\n",
    "        accuracy.append(res['accuracy'])\n",
    "        prediction.append(res['prediction'])\n",
    "        expected.append(blob.expect)\n",
    "    # Organize result arrays\n",
    "    accuracy = np.hstack(accuracy)\n",
    "    expected = np.hstack(expected)\n",
    "    prediction = np.hstack(prediction)\n",
    "    return accuracy, expected, prediction\n",
    "\n",
    "# For the training set:\n",
    "accuracy, label, prediction = evaluate(blob, trainLoader)\n",
    "print(\"TRAIN\\taccuracy mean\",accuracy.mean(),\"std\",accuracy.std())\n",
    "\n",
    "# For the testing set:\n",
    "accuracy, label, prediction = evaluate(blob, testLoader)\n",
    "print(\"TEST\\taccuracy mean\",accuracy.mean(),\"std\",accuracy.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YAY! My first real neural net!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
